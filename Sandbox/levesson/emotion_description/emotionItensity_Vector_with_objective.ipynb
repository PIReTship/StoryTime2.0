{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "\n",
    "# nltk is what does all the magic language parsing\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# for parsing wonky HTML-tag-cluttered descriptions\n",
    "import html2text\n",
    "from lxml import html\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# for making timing performance tests\n",
    "import time\n",
    "\n",
    "\n",
    "# for fetching book descriptions\n",
    "import requests\n",
    "from googlesearch import search\n",
    "\n",
    "# for stop words\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# On your first time running this program, you'll need to run the following line to download the stuff from nltk to your local machine\n",
    "# nltk.download(['stopwords', 'wordnet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Long stop words list using 3 different python stopword libraries\n",
    "\n",
    "stops = list(set(ENGLISH_STOP_WORDS)) + list(set(stopwords.words('english') +  list(set(STOP_WORDS)) + [\"http\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This Block loads the Lexicon and creates a data structure for the emotion intensity\n",
    "\n",
    "fileEmotion = \"emotion_itensity.txt\"\n",
    "table = pd.read_csv(fileEmotion,  names=[\"word\", \"emotion\", \"itensity\"], sep='\\t')\n",
    "\n",
    "#create the dictionary with the word/emotion/score\n",
    "emotion_dic = dict()\n",
    "lmtzr = WordNetLemmatizer()\n",
    "for index, row in table.iterrows():\n",
    "    #add first as it is given in the lexicon\n",
    "    temp_key = row['word'] + '#' + row['emotion']\n",
    "    emotion_dic[temp_key] = row['itensity']\n",
    "\n",
    "    #add in the normal noun form\n",
    "    temp_key_n = lmtzr.lemmatize(row['word']) + '#' + row['emotion']\n",
    "    emotion_dic[temp_key_n] = row['itensity']\n",
    "    \n",
    "    #add in the normal verb form\n",
    "    temp_key_v = lmtzr.lemmatize(row['word'], 'v') + '#' + row['emotion']\n",
    "    emotion_dic[temp_key_v] = row['itensity']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create h to clean descriptions in case they are in html format\n",
    "h = html2text.HTML2Text()\n",
    "h.ignore_links = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that get the emotion itensity\n",
    "def getEmotionItensity(word,emotion):\n",
    "    key = word + \"#\" + emotion\n",
    "    try:\n",
    "        return emotion_dic[key]\n",
    "    except:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the word is in the Lexicon\n",
    "def isWordInEmotionFile(word):\n",
    "    # Slightly faster implementation\n",
    "    for key in emotion_dic.keys():\n",
    "        if key.startswith(word + \"#\"):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "# print(isWordInEmotionFile(\"anger\"), isWordInEmotionFile(\"help\"), isWordInEmotionFile(\"booger\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopping checker \n",
    "def isStopWord(word):\n",
    "    if word in stops:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the emotion itensity to the dictionary\n",
    "def calculateEmotion(emotions, word):\n",
    "    emotions[\"Anger\"] += getEmotionItensity(word, \"anger\")\n",
    "    emotions[\"Anticipation\"] += getEmotionItensity(word, \"anticipation\")\n",
    "    emotions[\"Disgust\"] += getEmotionItensity(word, \"disgust\")\n",
    "    emotions[\"Fear\"] += getEmotionItensity(word, \"fear\")\n",
    "    emotions[\"Joy\"] += getEmotionItensity(word, \"joy\")\n",
    "    emotions[\"Sadness\"] += getEmotionItensity(word, \"sadness\")\n",
    "    emotions[\"Surprise\"] += getEmotionItensity(word, \"surprise\")\n",
    "    emotions[\"Trust\"] += getEmotionItensity(word, \"trust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the emotion vector of a given text\n",
    "def getEmotionVector(text, removeObj = False, useSynset = True):\n",
    "    #create the initial emotions\n",
    "    emotions = {\"Anger\": 0.0,\n",
    "                \"Anticipation\": 0.0,\n",
    "                \"Disgust\": 0.0,\n",
    "                \"Fear\": 0.0,\n",
    "                \"Joy\": 0.0,\n",
    "                \"Sadness\": 0.0,\n",
    "                \"Surprise\": 0.0,\n",
    "                \"Trust\": 0.0,\n",
    "                \"Objective\": 0.0}\n",
    "    #parse the description\n",
    "    str = re.sub(\"[^a-zA-Z]+\", \" \", text) # replace all non-letters with a space\n",
    "    pat = re.compile(r'[^a-zA-Z ]+')\n",
    "    str = re.sub(pat, '', str).lower() #  convert to lowercase\n",
    "\n",
    "    #split string\n",
    "    splits = str.split()\n",
    "\n",
    "    \n",
    "    #iterate over words array\n",
    "    for split in splits:\n",
    "        if not isStopWord(split):\n",
    "            #first check if the word appears as it does in the text\n",
    "            if isWordInEmotionFile(split): \n",
    "                calculateEmotion(emotions, split)\n",
    "                \n",
    "            # check the word in noun form (bats -> bat)\n",
    "            elif isWordInEmotionFile(lmtzr.lemmatize(split)):\n",
    "                calculateEmotion(emotions, lmtzr.lemmatize(split))\n",
    "                \n",
    "            # check the word in verb form (ran/running -> run)\n",
    "            elif isWordInEmotionFile(lmtzr.lemmatize(split, 'v')):\n",
    "                calculateEmotion(emotions, lmtzr.lemmatize(split, 'v'))  \n",
    "                \n",
    "            # check synonyms of this word\n",
    "            elif useSynset and wordnet.synsets(split) is not None:\n",
    "                # only check the first two \"senses\" of a word, so we don't stray too far from its intended meaning\n",
    "                for syn in wordnet.synsets(split)[0:1]:\n",
    "                    for l in syn.lemmas():\n",
    "                        if isWordInEmotionFile(l.name()):\n",
    "                            calculateEmotion(emotions, l.name())\n",
    "                            continue\n",
    "                            \n",
    "                # none of the synonyms matched something in the file\n",
    "                emotions[\"Objective\"] += 1\n",
    "                \n",
    "            else:\n",
    "                # not found in the emotion file, assign a score to Objective instead\n",
    "                emotions[\"Objective\"] += 1\n",
    "\n",
    "    # remove the Objective category if requested\n",
    "    if removeObj:\n",
    "        del emotions['Objective']\n",
    "        \n",
    "    total = sum(emotions.values())\n",
    "    for key in sorted(emotions.keys()):\n",
    "        try:\n",
    "            # normalize the emotion vector\n",
    "            emotions[key] = (1.0 / total) * emotions[key]\n",
    "        except:\n",
    "            emotions[key] = 0\n",
    "\n",
    "    return emotions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch a book's description from Apple Books given its name\n",
    "def fetchBookDesc(name):\n",
    "    # find the url via google\n",
    "    query = \"site:books.apple.com \" + name\n",
    "    returnSearch = search(query, tld='com', lang='en', num=1, stop=3, pause=0.5)\n",
    "    url = next(returnSearch)\n",
    "    \n",
    "    # need to pretend to be a user rather than a robot\n",
    "    headers = {    'User-Agent': 'My User Agent 1.0',}\n",
    "    page = requests.get(url, headers=headers)\n",
    "    page.raise_for_status()\n",
    "\n",
    "    # by knowing how apple books is laid out, we can find the div with the description\n",
    "    tree = html.fromstring(page.content)\n",
    "    frame = tree.xpath('//div[not(@class=\"we-lockup__title \")]/p[not(contains(text(),\"\\n\"))]/text()')\n",
    "    \n",
    "    # convert frame to a string description\n",
    "    return ' '.join(frame)\n",
    "    \n",
    "# fetchBookDesc(\"Harry Potter and the Chamber of Secrets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This block just test the functions above\n",
    "#open description file\n",
    "# file = open(\"description.txt\",\"r\")\n",
    "# str_f = file.read()\n",
    "# file.close()\n",
    "\n",
    "# fetch book description from Apple Books\n",
    "# str_f = fetchBookDesc(\"Stephen King's IT: novel\")\n",
    "# formatedDescription = h.handle(str_f)\n",
    "# results = getEmotionVector(formatedDescription)\n",
    "\n",
    "# print(results)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "\n",
    "# Graphs the emotion vector as a horizontal bar graph\n",
    "def graphVector(results, title=\"Emotion Intensity\", hasObjective = True):\n",
    "    r = dict(results)\n",
    "    if hasObjective:\n",
    "        plt.barh(range(len(r)), list(r.values()), align='center')\n",
    "        plt.yticks(range(len(r)), list(r.keys()))\n",
    "    \n",
    "        #plt.xlabel('Emotion')\n",
    "        plt.title(title + ' with Objective')\n",
    "        plt.show()\n",
    "        \n",
    "        del r['Objective']\n",
    "        \n",
    "    plt.barh(range(len(r)), list(r.values()), align='center')\n",
    "    plt.yticks(range(len(r)), list(r.keys()))\n",
    "\n",
    "    #plt.xlabel('Emotion')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# graphVector(results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#This block run over all the books on the CSV file\n",
    "#before executing this cell, make sure the csv file has a column called \"emotion_NRC_objective\"\n",
    "\n",
    "# THIS WILL TAKE A WHILE\n",
    "csv_file = 'complete_pl.csv'\n",
    "books = pd.read_csv(csv_file)\n",
    "for index, row in books.iterrows():\n",
    "    description = h.handle(row['description'])\n",
    "    books.loc[index, 'emotion_NRC_objective'] = str(getEmotionVector(description))\n",
    "#output file    \n",
    "books.to_csv('complete_pl_output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a vector for every book in the csv file, then output those vectors to another file\n",
    "# Only needs to be run once per file\n",
    "# THIS WILL TAKE A WHILE\n",
    "ratings_file = '../../../Data/Teenager_GoodReads.csv'\n",
    "books = pd.read_csv(ratings_file)\n",
    "# initialize the columns in the dataframe where our emotion vectors go\n",
    "books[\"no_synset\"] = \"\" * len(books.index)\n",
    "books[\"synset\"] = \"\" * len(books.index)\n",
    "\n",
    "# add all the vectors to a csv file\n",
    "sampleSize = 2000\n",
    "startIndex = 3020\n",
    "\n",
    "for index in range(startIndex, startIndex + sampleSize):\n",
    "   \n",
    "    d = books.loc[index, 'description']\n",
    "    if d is None or type(d) is not str:\n",
    "        continue\n",
    "        \n",
    "    description = h.handle(d)\n",
    "#     print(index, description)\n",
    "    \n",
    "    books.loc[index, 'no_synset'] = str(getEmotionVector(description, useSynset = False))\n",
    "    books.loc[index, 'synset'] = str(getEmotionVector(description))\n",
    "#output file    \n",
    "books.to_csv('ratings_teen_processed.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import math\n",
    "# THIS WILL TAKE A WHILE\n",
    "# TEENAGERS\n",
    "# run over all ratings and get the ones with these params's averaged emotional vector\n",
    "# ratings_file = '../../../Data/.csv'\n",
    "\n",
    "\n",
    "# print(ratings)\n",
    "\n",
    "avgVector = getEmotionVector(\"\")\n",
    "for index, row in ratings.iterrows():\n",
    "#     print(row[\"title\"])\n",
    "    if math.floor(row[\"age\"]) in range(18,20): # edit this for age\n",
    "        r = row[\"avg_rating\"]\n",
    "        if r is None or type(r) not in [int, float] or math.isnan(r):\n",
    "            continue\n",
    "            \n",
    "        if math.floor(r) in range(0, 5): # edit this for ratings \n",
    "            # make sure the discription exists, otherwise we discount it\n",
    "            d = row[\"description\"]\n",
    "            if d is None or type(d) is not str:\n",
    "                continue\n",
    "#                 print(d)\n",
    "            # description exists\n",
    "            description = h.handle(d)\n",
    "            v = getEmotionVector(description)\n",
    "            for key in avgVector:\n",
    "                avgVector[key] += v[key]\n",
    "\n",
    "graphVector(avgVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphDictionary(dict, desc = \"\"):\n",
    "    graphVector(dict[\"12-13\"], \"12-13\" + desc)\n",
    "    graphVector(dict[\"14-15\"], \"14-15\" + desc)\n",
    "    graphVector(dict[\"16-17\"], \"16-17\" + desc)\n",
    "    graphVector(dict[\"18+\"], \"18-19\" + desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "ratings_file = '../../../Data/Teenager_GoodReads.csv' # teenager review data\n",
    "ratings = pd.read_csv(ratings_file)\n",
    "\n",
    "rat_12 = ratings[ratings[\"age\"] == 12]\n",
    "\n",
    "ageRatings = {\n",
    "    12: [],\n",
    "    13: [],\n",
    "    14: [],\n",
    "    15: [],\n",
    "    16: [],\n",
    "    17: [],\n",
    "    18: [],\n",
    "    19: []\n",
    "}\n",
    "for age in range(12, 20):\n",
    "    ageRatings[age] = ratings[(ratings[\"age\"] >= age) & (ratings['age'] < age+1)]\n",
    "\n",
    "# ageRatings[14].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the graph representing the books rated between min and max rating, considering specified ages (list)\n",
    "def makeGraphs(ages, minRating=0, maxRating=5):\n",
    "    print(ages,minRating,maxRating)\n",
    "    avgVector = getEmotionVector(\"\")\n",
    "\n",
    "    for age in ages:\n",
    "        print(\"Parsing data for \", age, \" year-olds\")\n",
    "        data = ageRatings[age]\n",
    "        filtered = data[(data[\"avg_rating\"] >= minRating) & (data[\"avg_rating\"] <= maxRating)]\n",
    "        \n",
    "        limit = 100 # only select a sample of this many\n",
    "        currIter = 0\n",
    "        for index, row in filtered.iterrows():\n",
    "            if currIter >= limit:\n",
    "                break\n",
    "            currIter += 1\n",
    "\n",
    "            \n",
    "            d = row[\"description\"]\n",
    "            if d is None or type(d) is not str:\n",
    "                continue\n",
    "\n",
    "            # description exists\n",
    "            description = h.handle(d)\n",
    "            v = getEmotionVector(description)\n",
    "            for key in avgVector:\n",
    "                avgVector[key] += v[key]\n",
    "    \n",
    "    return avgVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noObj(v):\n",
    "    r = dict(v)\n",
    "    del r[\"Objective\"]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphDifference(a, b, title = \"difference\"):\n",
    "    # graph the difference between two emotion vectors\n",
    "    dif = b - a\n",
    "    emotions = {\"Anger\": dif[0],\n",
    "            \"Anticipation\": dif[1],\n",
    "            \"Disgust\": dif[2],\n",
    "            \"Fear\": dif[3],\n",
    "            \"Joy\": dif[4],\n",
    "            \"Sadness\": dif[5],\n",
    "            \"Surprise\": dif[6],\n",
    "            \"Trust\": dif[7]}\n",
    "    if len(dif) > 8:\n",
    "        emotions[\"Objective\"] = dif[8]\n",
    "    # visually show the difference\n",
    "    graphVector(emotions, title, False)\n",
    "    return dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorMatch(zero, compare, shouldGraph = True, title = \"difference\"):\n",
    "    # compare how close 'zero' and 'compare' are to each other visually\n",
    "#     graphVector(zero, 'samp', False)\n",
    "#     graphVector(compare, 'comp', False)\n",
    "    # convert to np arrays\n",
    "    zero = np.array(list(zero.values()))\n",
    "    compare = np.array(list(compare.values()))\n",
    "    \n",
    "    # normalize both vectors\n",
    "    zero = zero / np.linalg.norm(zero)\n",
    "    compare = compare / np.linalg.norm(compare)\n",
    "    \n",
    "    # graph the difference graph\n",
    "    if shouldGraph:\n",
    "        graphDifference(zero, compare, title)\n",
    "    \n",
    "    # return the magnitude of the vector of the difference\n",
    "    return np.linalg.norm(compare - zero)\n",
    "\n",
    "# experiment shows that objective should largely be ignored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DICTIONARY STRINGS\n",
    "# See Also: Appendix II of our paper, located at the root of this github repo\n",
    "\n",
    "# without synset\n",
    "veryGood = {'12-13': {'Anger': 0.02607737447252457, 'Anticipation': 0.05591565890156308, 'Disgust': 0.016045564608362352, 'Fear': 0.04789091036691487, 'Joy': 0.07800394549107047, 'Sadness': 0.02963231050132504, 'Surprise': 0.025570341420305114, 'Trust': 0.07646456858162831, 'Objective': 0.6443993256563062}, '14-15': {'Anger': 0.03133700242969171, 'Anticipation': 0.05306542575331012, 'Disgust': 0.02052466230415404, 'Fear': 0.050078575418488806, 'Joy': 0.07091271631929497, 'Sadness': 0.036037637580607344, 'Surprise': 0.024350939438084956, 'Trust': 0.07197318113344676, 'Objective': 0.6417198596229213}, '16-17': {'Anger': 0.0333773979974969, 'Anticipation': 0.05330467754243581, 'Disgust': 0.02097467965804083, 'Fear': 0.05641121602721728, 'Joy': 0.0717972294372776, 'Sadness': 0.04119477515402107, 'Surprise': 0.02393239572109254, 'Trust': 0.07262647668304764, 'Objective': 0.6263811517793704}, '18+': {'Anger': 0.027350619283685446, 'Anticipation': 0.04834423625870798, 'Disgust': 0.01810802229683925, 'Fear': 0.04711267423376138, 'Joy': 0.0682425285607569, 'Sadness': 0.03469139079167855, 'Surprise': 0.024225540160949112, 'Trust': 0.07372016783601339, 'Objective': 0.658204820577608}}\n",
    "medium = {'12-13': {'Anger': 0.029189140945891232, 'Anticipation': 0.051557508020711164, 'Disgust': 0.016899369038538357, 'Fear': 0.04755995823626276, 'Joy': 0.06660662293889227, 'Sadness': 0.03507920186496511, 'Surprise': 0.028374183831625363, 'Trust': 0.07083812291298776, 'Objective': 0.6538958922101261}, '14-15': {'Anger': 0.0285833109659729, 'Anticipation': 0.057618467957265156, 'Disgust': 0.01961486079752071, 'Fear': 0.052700259717779016, 'Joy': 0.07232740308860498, 'Sadness': 0.03513043731590352, 'Surprise': 0.02588909747908478, 'Trust': 0.07539679181497354, 'Objective': 0.6327393708628954}, '16-17': {'Anger': 0.025644625944838434, 'Anticipation': 0.051360156037244954, 'Disgust': 0.017925369540969096, 'Fear': 0.04640777241724535, 'Joy': 0.060824867086734236, 'Sadness': 0.03225630922943315, 'Surprise': 0.026661435749587997, 'Trust': 0.06426302689204114, 'Objective': 0.6746564371019056}, '18+': {'Anger': 0.02526227585581172, 'Anticipation': 0.04694168121197902, 'Disgust': 0.018564253736720836, 'Fear': 0.04646442057241248, 'Joy': 0.0634355612618883, 'Sadness': 0.03280124674603629, 'Surprise': 0.024291726419989793, 'Trust': 0.07112877986963899, 'Objective': 0.6711100543255224}}\n",
    "bad = {'12-13': {'Anger': 0.014256266830366897, 'Anticipation': 0.049176530976752016, 'Disgust': 0.007874240160354389, 'Fear': 0.025594917066862136, 'Joy': 0.06747528283150003, 'Sadness': 0.016159672424431983, 'Surprise': 0.02917186766565966, 'Trust': 0.07731285216349064, 'Objective': 0.7129783698805822}, '14-15': {'Anger': 0.014115226149470509, 'Anticipation': 0.04034092911328387, 'Disgust': 0.010038021527596707, 'Fear': 0.029141883181940137, 'Joy': 0.058447614793800576, 'Sadness': 0.01528371769742746, 'Surprise': 0.014531136480208482, 'Trust': 0.09533626642161397, 'Objective': 0.7227652046346582}, '16-17': {'Anger': 0.018530938051548828, 'Anticipation': 0.04952214921103868, 'Disgust': 0.010244400746641341, 'Fear': 0.03049261738092707, 'Joy': 0.06100403145970391, 'Sadness': 0.023510096408279352, 'Surprise': 0.017283416843489417, 'Trust': 0.0778980982594321, 'Objective': 0.7115142516389393}, '18+': {'Anger': 0.018685170909377744, 'Anticipation': 0.04940155601721161, 'Disgust': 0.016405495432130657, 'Fear': 0.035722493931139354, 'Joy': 0.06305838192496861, 'Sadness': 0.022540359501121345, 'Surprise': 0.016742206897730393, 'Trust': 0.07503267563743672, 'Objective': 0.7024116597488834}}\n",
    "\n",
    "# with synset\n",
    "# veryGood = {'12-13': {'Anger': 0.03324516731797181, 'Anticipation': 0.06316521380213953, 'Disgust': 0.022227091137027172, 'Fear': 0.05691485227312035, 'Joy': 0.08137360321373614, 'Sadness': 0.03561005495603787, 'Surprise': 0.02753410719569374, 'Trust': 0.08740544859699105, 'Objective': 0.5925244615072824}, '14-15': {'Anger': 0.03845659187984517, 'Anticipation': 0.05968186928716008, 'Disgust': 0.024811851649648775, 'Fear': 0.060174546666221615, 'Joy': 0.07738135360361288, 'Sadness': 0.041013853384338446, 'Surprise': 0.026242366975899992, 'Trust': 0.08078540253353338, 'Objective': 0.5914521640197395}, '16-17': {'Anger': 0.03937934022078974, 'Anticipation': 0.06168553740119403, 'Disgust': 0.02437694812017651, 'Fear': 0.06467916627690419, 'Joy': 0.07596938986336625, 'Sadness': 0.04700959580673139, 'Surprise': 0.02631626776151658, 'Trust': 0.08092024537924439, 'Objective': 0.579663509170077}, '18+': {'Anger': 0.032588571818511436, 'Anticipation': 0.056495245508095024, 'Disgust': 0.021237897122595596, 'Fear': 0.054496669724892546, 'Joy': 0.07277402651940436, 'Sadness': 0.039143449052118896, 'Surprise': 0.025819972212946407, 'Trust': 0.08610576994731546, 'Objective': 0.6113383980941202}} \n",
    "# medium = {'12-13': {'Anger': 0.03374098128626937, 'Anticipation': 0.06018846633940684, 'Disgust': 0.019014375576554318, 'Fear': 0.054401185770716745, 'Joy': 0.07337335029206232, 'Sadness': 0.04040692892957006, 'Surprise': 0.030654045663137744, 'Trust': 0.08191808222017224, 'Objective': 0.6063025839221104}, '14-15': {'Anger': 0.03359076268594646, 'Anticipation': 0.06787135639254616, 'Disgust': 0.023418184759612503, 'Fear': 0.06048732307267755, 'Joy': 0.07937567941882766, 'Sadness': 0.04013902205286797, 'Surprise': 0.02947897436360671, 'Trust': 0.08800879300797261, 'Objective': 0.5776299042459424}, '16-17': {'Anger': 0.030456968279124176, 'Anticipation': 0.059058054919972264, 'Disgust': 0.02198290389875575, 'Fear': 0.05303760271253788, 'Joy': 0.0657024268770311, 'Sadness': 0.036185701108150475, 'Surprise': 0.029020076627564884, 'Trust': 0.0760225808088485, 'Objective': 0.628533684768015}, '18+': {'Anger': 0.031729533860877006, 'Anticipation': 0.05606660712348232, 'Disgust': 0.022249649499262326, 'Fear': 0.05366896026485633, 'Joy': 0.06871372152832912, 'Sadness': 0.036852106253912366, 'Surprise': 0.025940952655220298, 'Trust': 0.08186879744428478, 'Objective': 0.6229096713697754}} \n",
    "# bad = {'12-13': {'Anger': 0.017655532155044198, 'Anticipation': 0.05744157750390466, 'Disgust': 0.008167936076363206, 'Fear': 0.030417773390982866, 'Joy': 0.07423649832532445, 'Sadness': 0.017440343259113648, 'Surprise': 0.02993467844844481, 'Trust': 0.08817932713834818, 'Objective': 0.676526333702474}, '14-15': {'Anger': 0.02020149395761844, 'Anticipation': 0.047565525231736176, 'Disgust': 0.014517292449512638, 'Fear': 0.03547664051539909, 'Joy': 0.06533833356520834, 'Sadness': 0.021882118715121295, 'Surprise': 0.017664024997972833, 'Trust': 0.1132199386684898, 'Objective': 0.6641346318989414}, '16-17': {'Anger': 0.023330231392691952, 'Anticipation': 0.05884361939206312, 'Disgust': 0.014104841767633436, 'Fear': 0.037962854154754415, 'Joy': 0.06730466563882047, 'Sadness': 0.02811541783833771, 'Surprise': 0.018701038338162976, 'Trust': 0.0913024999787434, 'Objective': 0.6603348314987926}, '18+': {'Anger': 0.02356251104255825, 'Anticipation': 0.05656267181417433, 'Disgust': 0.019254852068808205, 'Fear': 0.042011270869920375, 'Joy': 0.07456356532678587, 'Sadness': 0.0259438858942028, 'Surprise': 0.018840096630702702, 'Trust': 0.08805987687649008, 'Objective': 0.6512012694763574}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# THIS WILL TAKE A WHILE\n",
    "\n",
    "# initialize dicts for rating ranges, defined below\n",
    "veryGood = {\n",
    "    \"12-13\": {},\n",
    "    \"14-15\":{},\n",
    "    \"16-17\":{},\n",
    "    \"18+\":{}\n",
    "}\n",
    "medium = {\n",
    "    \"12-13\": {},\n",
    "    \"14-15\":{},\n",
    "    \"16-17\":{},\n",
    "    \"18+\":{}\n",
    "}\n",
    "bad = {\n",
    "    \"12-13\": {},\n",
    "    \"14-15\":{},\n",
    "    \"16-17\":{},\n",
    "    \"18+\":{}\n",
    "}\n",
    "\n",
    "\n",
    "# Very Good: 4-5\n",
    "# Medium: 2.5-4\n",
    "# Bad: 0-2.5\n",
    "\n",
    "minRating = 4\n",
    "print(\"Very Good\")\n",
    "# print(\"Sampling 12-13\")\n",
    "veryGood[\"12-13\"] = makeGraphs([12,13], minRating)\n",
    "# print(\"Sampling 14-15\")\n",
    "veryGood[\"14-15\"] = makeGraphs([14,15], minRating)\n",
    "# print(\"Sampling 16-17\")\n",
    "veryGood[\"16-17\"] = makeGraphs([16,17], minRating)\n",
    "# print(\"Sampling 18+\")\n",
    "veryGood[\"18+\"] = makeGraphs([18,19], minRating)\n",
    "\n",
    "print(\"Medium\")\n",
    "minRating = 2.5\n",
    "maxRating = 4\n",
    "# print(\"Sampling 12-13\")\n",
    "medium[\"12-13\"] = makeGraphs([12,13], minRating, maxRating)\n",
    "# print(\"Sampling 14-15\")\n",
    "medium[\"14-15\"] = makeGraphs([14,15], minRating, maxRating)\n",
    "# print(\"Sampling 16-17\")\n",
    "medium[\"16-17\"] = makeGraphs([16,17], minRating, maxRating)\n",
    "# print(\"Sampling 18+\")\n",
    "medium[\"18+\"] = makeGraphs([18,19], minRating, maxRating)\n",
    "\n",
    "print(\"Bad\")\n",
    "minRating = 0\n",
    "maxRating = 2.5\n",
    "# print(\"Sampling 12-13\")\n",
    "bad[\"12-13\"] = makeGraphs([12,13], minRating, maxRating)\n",
    "# print(\"Sampling 14-15\")\n",
    "bad[\"14-15\"] = makeGraphs([14,15], minRating, maxRating)\n",
    "# print(\"Sampling 16-17\")\n",
    "bad[\"16-17\"] = makeGraphs([16,17], minRating, maxRating)\n",
    "# print(\"Sampling 18+\")\n",
    "bad[\"18+\"] = makeGraphs([18,19], minRating, maxRating)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
